{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataPrepros_Template.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4AsRsLrt4feN9ZqQlwKXh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisguti27/Notebooks/blob/main/AI/Machine%20Learning/Preprocesado%20de%20Datos/DataPrepros_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljakns3jQGf2"
      },
      "source": [
        "!cd ..\n",
        "!ls\n",
        "!git clone https://github.com/chrisguti27/Notebooks.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDCF4A30Ra7_"
      },
      "source": [
        "# Importar Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kcnxkuLRZwa"
      },
      "source": [
        "Data set: Conjunto de datos que van a ser usados para desarrollar el algoritmo de machine learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCXW9yYNK-yr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as  pd #libreria para importar datasets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "759wj_mIXdjQ"
      },
      "source": [
        "Data frame: Cuando el *Data set* se presenta en forma una tabla \".csv\", o un *array* bidimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tg57g5pQmrk",
        "outputId": "8bd117e0-a016-4ed1-ba78-62a80a7fd2eb"
      },
      "source": [
        "dataset = pd.read_csv(\"Notebooks/AI/Machine Learning/Preprocesado de Datos/Data.csv\")\n",
        "print(\"--------------Dataset-------------- \\n\",dataset,\n",
        "      \"\\n-----------------------------------\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------Dataset-------------- \n",
            "    Country   Age   Salary Purchased\n",
            "0   France  44.0  72000.0        No\n",
            "1    Spain  27.0  48000.0       Yes\n",
            "2  Germany  30.0  54000.0        No\n",
            "3    Spain  38.0  61000.0        No\n",
            "4  Germany  40.0      NaN       Yes\n",
            "5   France  35.0  58000.0       Yes\n",
            "6    Spain   NaN  52000.0        No\n",
            "7   France  48.0  79000.0       Yes\n",
            "8  Germany  50.0  83000.0        No\n",
            "9   France  37.0  67000.0       Yes \n",
            "-----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdwi4nQuYpUk"
      },
      "source": [
        "El data set, en este caso data frame, se divide en variables independientes o variables predictoras *X* y variables dependientes *Y*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsJEOgZFRJ2p",
        "outputId": "f3b31b96-fb90-476e-ae05-abc0b3a86981"
      },
      "source": [
        "X = dataset.iloc[:,:-1].values  #.iloc index localization\n",
        "Y = dataset.iloc[:,-1].values   #-1 ultima columna no se incluye en el dato\n",
        "                                #.values extraer solo los valores y no las posiciones\n",
        "print(\"Variables independientes\\n\", X,\"\\n Variables dependientes\\n\",Y)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variables independientes\n",
            " [['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]] \n",
            " Variables dependientes\n",
            " ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZbALZ1pT7yy"
      },
      "source": [
        "# Tratamiento de los NAs o datos faltantes/desconocidos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imK6CCNiWuxq"
      },
      "source": [
        "En ciertos casos pueden faltar uno o varios datos en el dataset, es decir que encontraremos valores *NaN*. Considerando que al borrar las filas con valores *NaN* perderíamos información importante para modelar el algoritmo. Es necesario aplicar métodos para asignar un valor correspondiente que reemplace este valor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Qiol55T_Ii",
        "outputId": "54c97d2e-b8aa-4390-958e-e44f12d35676"
      },
      "source": [
        "from sklearn.impute import SimpleImputer #llama a la funcion Imputer\n",
        "                                         # biblioteca de preprosemiento de datos\n",
        "imputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\", verbose = 0)\n",
        "#missing_values : valores de las celdas sin informacion\n",
        "#strategy : estrategia para reemplazar el valor de dicha celda, \n",
        "            #\"mean\" ----> mediana\n",
        "#verbose: respecto a que eje se va a realizar strategy,\n",
        "            #0: la accion strategy de la columna\n",
        "            #1: la accion strategy de la fila\n",
        "imputer.fit(X[:,1:3])#la ultima columna en el corchete no esta incluido\n",
        "#se reemplazan los valores NAN por los valores de la funcion imputer\n",
        "X[:,1:3] = imputer.transform(X[:,1:3])\n",
        "print(\"----Data set sin NaNs-----\\n\",X)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----Data set sin NaNs-----\n",
            " [['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KflnyAU8cgoF"
      },
      "source": [
        "# Datos categóricos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHvNF7bWc69_"
      },
      "source": [
        "Los datos o variables categóricas son etiquetas, valores cualitativos, es decir valores no ordinales. También se las conoce como variables dummy.\n",
        "\n",
        "En el procesamiento de datos para machine learning hay que señalar la importancia de manejar datos numéricos para aplicar o desarrollar algoritmos de modelamiento. Por lo que, estas variables dummy deben ser sometidas a un procesamiento para expresar categoría en una representación numérica. Para ello se emplea el determinado *one hot encoding*, que se trata de tranformar la variables, valores de la columna *Country* y *Purchased*, en tantas columnas como categorías existan.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inSp7oAZccaS",
        "outputId": "bccf6656-597b-4d30-bad6-8b026030b38f"
      },
      "source": [
        "#codificar datos categoricos\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "labelencoder_X = LabelEncoder()\n",
        "X[:,0] = labelencoder_X.fit_transform(X[:,0])\n",
        "\n",
        "#one hot encoding para variales categoricas, sin valor ordinal\n",
        "ct = ColumnTransformer(\n",
        "    [('one_hot_encoder', OneHotEncoder(categories='auto'), [0])],    # The column numbers to be transformed (here is [0] but can be [0, 1, 3])\n",
        "    remainder='passthrough'                         # Leave the rest of the columns untouched\n",
        ")\n",
        "\n",
        "X = np.array(ct.fit_transform(X), dtype=np.int32)\n",
        "\n",
        "print(\"Codificacion de Datos Categoricos para X\\n\",X)\n",
        "\n",
        "labelencoder_Y = LabelEncoder()\n",
        "Y = labelencoder_Y.fit_transform(Y)\n",
        "print(\"Codificacion de Datos Categoricos para Y\\n\",Y)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Codificacion de Datos Categoricos para X\n",
            " [[    1     0     0    44 72000]\n",
            " [    0     0     1    27 48000]\n",
            " [    0     1     0    30 54000]\n",
            " [    0     0     1    38 61000]\n",
            " [    0     1     0    40 63777]\n",
            " [    1     0     0    35 58000]\n",
            " [    0     0     1    38 52000]\n",
            " [    1     0     0    48 79000]\n",
            " [    0     1     0    50 83000]\n",
            " [    1     0     0    37 67000]]\n",
            "Codificacion de Datos Categoricos para Y\n",
            " [0 1 0 0 1 1 0 1 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgty71GDgRXg"
      },
      "source": [
        "# Dividir dataset en entrenamiento y test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB3VUn2cgXZo"
      },
      "source": [
        "Dentro del algoritmo de machine learning se pueden encontrar dos fases, *entretanimiento* y *test*, donde del data set se dispone una determinada cantidad para cada fase, procurando que exista la suficiente cantidad de datos para evitar *overfitting*, es decir, asegurar que exista aprendizaje. El porcentaje óptimo se entra en una realación 20-30%:80-70%, siendo el mayor procentaje para la fase de entrenamiento.\n",
        "\n",
        "*overfitting*: cuando el modelo se ajusta solamente a los datos de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZxPoUHzgXzr",
        "outputId": "27f46988-5d11-4fae-ea71-754cbb614d8c"
      },
      "source": [
        "#Dividir el data set en conjunto de entrenamiento y conjunto de testing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train: v. independientes para entrenar el algoritmo\n",
        "#X_test: v. independientes para testing\n",
        "#Y_train: v. dependientes para entrenar el algoritmo\n",
        "#Y_test: v. dependientes para testi\n",
        "#train_test_split(X, Y, test_size = 0.2)\n",
        "    #X: matriz de v. indep, \n",
        "    #Y: vector de v. a predecir, \n",
        "    #test_size = % que se usaran para testing 20-30% para testing es lo normal\n",
        "    #mientras mas datos para entrenar tenga el algoritmo mejor sera la predicción\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
        "print(\"-------------+++X+++-------------\\n\",\"------------Training------------\\n\",X_train,\n",
        "      \"\\n----------------Test-------------\\n\", X_test, \n",
        "      \"\\n--------------+++Y+++------------\\n\",\"------------Training------------\\n\",Y_train,\n",
        "      \"\\n----------------Test-------------\\n\",Y_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------+++X+++-------------\n",
            " ------------Training------------\n",
            " [[    0     1     0    40 63777]\n",
            " [    1     0     0    37 67000]\n",
            " [    0     0     1    27 48000]\n",
            " [    0     0     1    38 52000]\n",
            " [    1     0     0    48 79000]\n",
            " [    0     0     1    38 61000]\n",
            " [    1     0     0    44 72000]\n",
            " [    1     0     0    35 58000]] \n",
            "----------------Test-------------\n",
            " [[    0     1     0    30 54000]\n",
            " [    0     1     0    50 83000]] \n",
            "--------------+++Y+++------------\n",
            " ------------Training------------\n",
            " [1 1 1 0 1 0 0 1] \n",
            "----------------Test-------------\n",
            " [0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaBcQnEAjTW0"
      },
      "source": [
        "# Escalamiento de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCFTga0TjaFR"
      },
      "source": [
        "Los valores dentro de un data set por lo general no se encuentran en el mismo rango numérico, es decir, puede ser que los valores de la variable *edad* esté en el rango de *0-100* mientras que los de purchased en el rango *30000 - 100000*. Por lo tanto, los valores de esta última variable tendrán un impacto mayor en el modelo de lo que los valores de la variable edad.\n",
        "\n",
        "Para evitar este escenario, que unas variables dominen sobre otras, se realiza un escalado que puede ser mediante  normalización o estandarización de los datos.\n",
        "\n",
        "Con esto se asegura que el propio algoritmo se encargue de disernir qué peso otorgar a cada variable para el modelado del data set, según se aporte al proceso de clasificación o predicción.\n",
        "\n",
        "---\n",
        "Para el caso de este data set, los valores de salida *Y* no se escalan porque el algoritmo es de clasificación.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwuFSWlplGay",
        "outputId": "ee4af4a4-09a4-45e2-9ffe-15ba982a12f9"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()#escalador de datos por STANDARISATION\n",
        "#se escala el conjunto de entrenamiento\n",
        "#con sc_X.fit_transform(X_train) el conjunto de entrenamiento se escala automaticamente\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "print(\"Variables de entrenamiento escaladas:\\n\",X_train)\n",
        "#para las variables test se emplea el mismo escalado que\n",
        "#para las variables train, por eso solamente se llama\n",
        "#al metodo sc_X.transform(X_test)\n",
        "X_test = sc_X.transform(X_test)\n",
        "print(\"Variables de test escaladas:\\n\",X_train)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variables de entrenamiento escaladas:\n",
            " [[-1.          2.64575131 -0.77459667  0.27978024  0.12374357]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.23673712  0.4617671 ]\n",
            " [-1.         -0.37796447  1.29099445 -1.95846165 -1.53092514]\n",
            " [-1.         -0.37796447  1.29099445 -0.06456467 -1.11141099]\n",
            " [ 1.         -0.37796447 -0.77459667  1.65715986  1.72030956]\n",
            " [-1.         -0.37796447  1.29099445 -0.06456467 -0.16750414]\n",
            " [ 1.         -0.37796447 -0.77459667  0.96847005  0.98615979]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.58108203 -0.48213975]]\n",
            "Variables de test escaladas:\n",
            " [[-1.          2.64575131 -0.77459667  0.27978024  0.12374357]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.23673712  0.4617671 ]\n",
            " [-1.         -0.37796447  1.29099445 -1.95846165 -1.53092514]\n",
            " [-1.         -0.37796447  1.29099445 -0.06456467 -1.11141099]\n",
            " [ 1.         -0.37796447 -0.77459667  1.65715986  1.72030956]\n",
            " [-1.         -0.37796447  1.29099445 -0.06456467 -0.16750414]\n",
            " [ 1.         -0.37796447 -0.77459667  0.96847005  0.98615979]\n",
            " [ 1.         -0.37796447 -0.77459667 -0.58108203 -0.48213975]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ2riMPYmbkg"
      },
      "source": [
        "Dentro de la fase de escalado se halla una discusión, acerca de sí escalar o no las variables dummy. No existe una sóla respuesta, ya que para cualquiera de los casos se puede dar razones de peso para inclinarse por una u otra postura. Todo dependerá del cirterio que quiera abordar el programador, que a su vez estará sujeto a cada situación y problemática que interpreten los datos.\n",
        "\n",
        "Lo que está claro, es que en cualquiera de las dos situaciones, *Sí* o *No*, la velocidad de convergencia del algoritmo no se verá afectada."
      ]
    }
  ]
}